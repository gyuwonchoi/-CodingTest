{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_with_examples.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPm3vv5JMfG4NEvu7NNefHv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyuwonchoi/-CodingTest/blob/main/pytorch_with_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 파이토치 특징\n",
        "* Tensor는 NumPy와 유사하지만 GPU 상에서 실행 가능\n",
        "* AutoGrad를 활용한 자동 미분\n",
        "\n",
        "이번 튜토리얼에서는 third order polynomial 3차 다항식을 활용하여 sin(x)에 근사하도록 함, 경사 하강법의 활용\n"
      ],
      "metadata": {
        "id": "YBEuPoGi10WE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor\n",
        "\n",
        "우선 NumPy를 활용하여 신경망을 구성한 뒤 텐서에 대한 이해\n",
        "\n",
        "* 넘파이는 computaion 그래프, gradient에 대하여 어려움"
      ],
      "metadata": {
        "id": "GiiPN4Du2Jgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math \n",
        "\n",
        "# 랜덤 입출력\n",
        "x = np.linspace(-math.pi, math.pi, 2000) # Return evenly spaced numbers over a specified interval\n",
        "                                         # Stard, End, num of sample\n",
        "y = np.sin(x) # G.T\n",
        "\n",
        "# 가중치 초기화, 3차 다항식 이므로 계수 4개 필요 \n",
        "a = np.random.randn()\n",
        "b = np.random.randn()\n",
        "c = np.random.randn()\n",
        "d = np.random.randn()\n",
        "\n",
        "learning_rate= 1e-6\n",
        "\n",
        "for t in range(2300):\n",
        "    y_pred=a + b*x + c*x**2 + d*x**3\n",
        "\n",
        "    loss = np.square(y_pred - y).sum() # 제곱으로 차\n",
        "    if t%100==99:\n",
        "      print(f\"iter:{t} , loss: {loss}\")\n",
        "\n",
        "    # 그려보면 다 맞다 \n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # 가중치를 갱신합니다.\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf1aRoIE2PWh",
        "outputId": "385b7e5e-a6a1-4af3-a722-cb05337fc4b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter:99 , loss: 1534.637531517626\n",
            "iter:199 , loss: 1068.696797078879\n",
            "iter:299 , loss: 745.704968280226\n",
            "iter:399 , loss: 521.5897919274852\n",
            "iter:499 , loss: 365.93719621871134\n",
            "iter:599 , loss: 257.73533681680493\n",
            "iter:699 , loss: 182.45286095530923\n",
            "iter:799 , loss: 130.0299441483298\n",
            "iter:899 , loss: 93.4953982929824\n",
            "iter:999 , loss: 68.01369206123735\n",
            "iter:1099 , loss: 50.227518476460084\n",
            "iter:1199 , loss: 37.80376669099\n",
            "iter:1299 , loss: 29.11963736242585\n",
            "iter:1399 , loss: 23.045420974630712\n",
            "iter:1499 , loss: 18.794020188052915\n",
            "iter:1599 , loss: 15.81660626828327\n",
            "iter:1699 , loss: 13.730196691844386\n",
            "iter:1799 , loss: 12.267341974275404\n",
            "iter:1899 , loss: 11.241140826955444\n",
            "iter:1999 , loss: 10.520892619197106\n",
            "iter:2099 , loss: 10.015138610330823\n",
            "iter:2199 , loss: 9.659840118499314\n",
            "iter:2299 , loss: 9.410131189799584\n",
            "Result: y = -0.02471361389809484 + 0.8634641835777135 x + 0.004263509633731696 x^2 + -0.09428672567991854 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor\n"
      ],
      "metadata": {
        "id": "Wu-JbHW7534x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math \n",
        "\n",
        "dtype = torch.float\n",
        "device= torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# AutoGrad를 위하여 requries_grad를 True로 : backward 하면 자동으로 a 객체 내 grad 로 미분값 계산\n",
        "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # loss는 loss function이고 이에 대한 식을 기준으로 auto grad를 함\n",
        "    # loss.backward로 표현 \n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # weight 업데이트 시에는 gradient 계산 않으므로\n",
        "    # 이걸 optimizer와 optim.step()으로 대체 \n",
        "    with torch.no_grad():\n",
        "        a -= learning_rate * a.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        c -= learning_rate * c.grad\n",
        "        d -= learning_rate * d.grad\n",
        "\n",
        "        # 매 iter마다 gradient 새로 계산 \n",
        "        a.grad = None\n",
        "        b.grad = None\n",
        "        c.grad = None\n",
        "        d.grad = None\n",
        "    \n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fcs-J8gf57u8",
        "outputId": "2e911736-1eb8-4bc5-e3cc-186ae24dc680"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 543.2364501953125\n",
            "199 377.4659729003906\n",
            "299 263.3881530761719\n",
            "399 184.79641723632812\n",
            "499 130.59274291992188\n",
            "599 93.16914367675781\n",
            "699 67.30377197265625\n",
            "799 49.40827941894531\n",
            "899 37.014617919921875\n",
            "999 28.422710418701172\n",
            "1099 22.46063995361328\n",
            "1199 18.319625854492188\n",
            "1299 15.440839767456055\n",
            "1399 13.437801361083984\n",
            "1499 12.042908668518066\n",
            "1599 11.070743560791016\n",
            "1699 10.392655372619629\n",
            "1799 9.919336318969727\n",
            "1899 9.58869457244873\n",
            "1999 9.357573509216309\n",
            "Result: y = 0.02272731438279152 + 0.8480797410011292 x + -0.0039208391681313515 x^2 + -0.0920984223484993 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " optimizer.zero_grad()\n",
        "\n",
        "    # 역전파 단계: 모델의 매개변수들에 대한 손실의 변화도를 계산합니다.\n",
        "loss.backward()\n",
        "\n",
        "    # optimizer의 step 함수를 호출하면 매개변수가 갱신됩니다.\n",
        "optimizer.step()\n"
      ],
      "metadata": {
        "id": "PrlVT8Os_mxV"
      }
    }
  ]
}